# Model-specific configuration for different LLM providers

ollama:
  models:
    llama3.2:
      context_window: 8192
      supports_functions: false
    llama3.1:
      context_window: 128000
      supports_functions: true
    mistral:
      context_window: 32768
      supports_functions: false

openai:
  models:
    gpt-4:
      context_window: 8192
      supports_functions: true
      cost_per_1k_tokens: 0.03
    gpt-3.5-turbo:
      context_window: 16384
      supports_functions: true
      cost_per_1k_tokens: 0.0015
    
  rate_limits:
    requests_per_minute: 3500
    tokens_per_minute: 90000
