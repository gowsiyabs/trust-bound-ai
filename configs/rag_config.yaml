# RAG System Configuration
#
# Cost-efficient OpenAI setup (recommended):
#   In .env set:
#     LLM_PROVIDER=openai
#     LLM_MODEL=gpt-4o-mini
#     OPENAI_API_KEY=sk-...
#     EMBEDDING_PROVIDER=openai       # uses text-embedding-3-small
#
# The embedding provider is auto-detected from OPENAI_API_KEY if
# EMBEDDING_PROVIDER is not explicitly set.
# ─────────────────────────────────────────────────────────────────────────────

embeddings:
  # Provider is overridden by EMBEDDING_PROVIDER env var (or auto-detected).
  # "huggingface" = free local compute; "openai" = $0.02/1M tokens
  provider: "huggingface"

  # HuggingFace (local)
  model_name: "BAAI/bge-large-en-v1.5"
  device: "cpu"          # Change to "cuda" if GPU available
  batch_size: 32

  # OpenAI (cloud, recommended)
  openai_model: "text-embedding-3-small"
  # openai_dimensions: 512  # Optional: smaller vectors = lower storage cost

chunking:
  # 512 tokens ≈ 1 focused idea per chunk.
  # Smaller chunks = cheaper embeddings + tighter context in generation prompt.
  chunk_size: 512
  chunk_overlap: 100

retrieval:
  # 5 chunks × ~512 tokens = ~2,560 context tokens per query.
  # Increasing top_k raises generation cost with diminishing returns.
  top_k: 5
  similarity_threshold: 0.7
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  use_hybrid_search: true
  bm25_weight: 0.3

llm:
  # ── These are the fallback defaults. ───────────────────────────────────────
  # To switch providers without editing this file, set env vars in .env:
  #
  #   Use OpenAI (recommended):
  #     LLM_PROVIDER=openai
  #     LLM_MODEL=gpt-4o-mini          # fast, cheap, good quality
  #     OPENAI_API_KEY=sk-...
  #
  #   Use Ollama (local, free):
  #     LLM_PROVIDER=ollama
  #     LLM_MODEL=llama3.2
  #
  #   Auto-detect: if OPENAI_API_KEY is set and LLM_PROVIDER is not,
  #   the system automatically uses OpenAI.
  # ───────────────────────────────────────────────────────────────────────────
  provider: "ollama"
  model: "llama3.2"
  temperature: 0.1
  max_tokens: 512          # 512 is enough for concise 10-K answers
  base_url: "http://127.0.0.1:11434"

vector_store:
  store_type: "chroma"
  collection_name: "rag_docs"
