# RAG Evaluation Lab - Health Insurance Q&A System

A production-ready RAG (Retrieval-Augmented Generation) system for health insurance policy Q&A with comprehensive AI trust & safety evaluation framework.

## ğŸ¯ Project Overview

This project transforms notebook-based RAG experiments into a complete production system featuring:

- **Advanced RAG Pipeline**: Multi-stage retrieval with query rewriting, hybrid search (semantic + BM25), and BGE-Large embeddings
- **AI Trust & Safety Layer**: Input validation, output filtering, hallucination detection, PII prevention
- **Comprehensive Evaluation**: Adversarial testing, safety metrics, RAG quality metrics
- **Interactive UI**: Streamlit chat interface with source citations and confidence indicators
- **Model Comparison**: Framework for comparing RAG vs fine-tuned models

## ğŸ—ï¸ Architecture

### System Overview

```mermaid
graph TB
    User[User Query] --> UI[Streamlit UI]
    UI --> Safety[Safety Layer]
    Safety --> Router{Route Query}
    Router -->|RAG Path| RAG[RAG System]
    Router -->|Finetuned Path| FT[Fine-tuned Model]
    RAG --> VectorDB[(Vector Store BGE-Large)]
    RAG --> LLM1[LLM Ollama/OpenAI]
    FT --> LLM2[Fine-tuned Model]
    RAG --> EvalRAG[RAG Evaluator]
    FT --> EvalFT[FT Evaluator]
    EvalRAG --> Compare[Comparison Dashboard]
    EvalFT --> Compare
    Safety --> Monitor[Safety Monitor]
    Monitor --> Logging[(Logs & Metrics)]
```

### RAG Pipeline Flow

```mermaid
graph LR
    Query[User Query] --> InputVal[Input Validator]
    InputVal -->|Valid| Rewrite[Query Rewriter]
    InputVal -->|Invalid| Reject[Reject Query]
    
    Rewrite --> Semantic[Semantic Search BGE-Large]
    Rewrite --> BM25[BM25 Search]
    
    Semantic --> Fusion[Score Fusion]
    BM25 --> Fusion
    
    Fusion --> Rerank[Cross-Encoder Reranking]
    Rerank --> TopK[Top-K Selection]
    
    TopK --> LLM[LLM Generation]
    LLM --> OutputFilter[Output Filter]
    
    OutputFilter -->|Safe| Response[Return Response]
    OutputFilter -->|Unsafe| Block[Block Response]
```

### Safety Layer Architecture

```mermaid
graph TB
    subgraph InputValidation [Input Validation]
        IQ[Query] --> JP[Jailbreak Detector]
        IQ --> MP[Medical Detector]
        IQ --> PP[PII Extractor Detector]
        IQ --> MP2[Malicious Pattern Detector]
    end
    
    subgraph OutputFiltering [Output Filtering]
        Resp[Response] --> HD[Hallucination Detector]
        Resp --> PD[PII Detector]
        Resp --> MC[Medical Advice Checker]
        Resp --> HC[Harmful Content Filter]
    end
    
    JP --> Decision{All Pass?}
    MP --> Decision
    PP --> Decision
    MP2 --> Decision
    
    HD --> FinalCheck{Safe?}
    PD --> FinalCheck
    MC --> FinalCheck
    HC --> FinalCheck
    
    Decision -->|Yes| Allow[Allow Query]
    Decision -->|No| Block1[Block Query]
    FinalCheck -->|Yes| Return[Return Response]
    FinalCheck -->|No| Block2[Block/Modify Response]
```

## ğŸ“ Project Structure

```
rag-eval-lab/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/                    # RAG pipeline components
â”‚   â”‚   â”œâ”€â”€ document_loader.py  # PDF ingestion with metadata extraction
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # BGE-Large embedding manager
â”‚   â”‚   â”œâ”€â”€ vector_store.py     # ChromaDB integration
â”‚   â”‚   â”œâ”€â”€ retriever.py        # Hybrid retriever (semantic + BM25)
â”‚   â”‚   â””â”€â”€ query_engine.py     # Insurance Q&A engine
â”‚   â”œâ”€â”€ safety/                 # AI Trust & Safety
â”‚   â”‚   â”œâ”€â”€ input_validator.py  # Jailbreak & adversarial detection
â”‚   â”‚   â””â”€â”€ output_filter.py    # Hallucination & PII detection
â”‚   â”œâ”€â”€ evals/                  # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ adversarial_tests.py  # 50+ adversarial test cases
â”‚   â”‚   â”œâ”€â”€ safety_metrics.py     # ASR, FPR, hallucination tracking
â”‚   â”‚   â”œâ”€â”€ rag_metrics.py        # Faithfulness, relevance, context quality
â”‚   â”‚   â”œâ”€â”€ comparison.py         # RAG vs Fine-tuned comparison
â”‚   â”‚   â””â”€â”€ continuous_eval.py    # Automated testing schedule
â”‚   â”œâ”€â”€ models/                 # Model factories
â”‚   â”‚   â”œâ”€â”€ llm_factory.py      # Ollama/OpenAI LLM factory
â”‚   â”‚   â””â”€â”€ rag_model.py        # Complete RAG pipeline wrapper
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ config.py           # Pydantic configuration
â”‚       â””â”€â”€ logging.py          # Structured logging
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ insurance_chat/         # Streamlit applications
â”‚       â”œâ”€â”€ app.py              # Chat interface
â”‚       â””â”€â”€ pages/
â”‚           â””â”€â”€ eval_dashboard.py  # Metrics visualization
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/              # Place your documents here (PDFs, text files)
â”‚   â”œâ”€â”€ eval_datasets/
â”‚   â”‚   â””â”€â”€ golden_qa.csv       # 50 insurance Q&A test cases
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ embeddings/         # Vector store persistence
â”œâ”€â”€ configs/                    # YAML configurations
â”‚   â”œâ”€â”€ rag_config.yaml
â”‚   â”œâ”€â”€ safety_config.yaml
â”‚   â””â”€â”€ model_config.yaml
â”œâ”€â”€ notebooks/                  # Original experiments (preserved)
â”œâ”€â”€ tests/                      # Unit tests
â””â”€â”€ results/                    # Evaluation results

```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/yourusername/rag-eval-lab.git
cd rag-eval-lab

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env
# Edit .env with your API keys (optional for OpenAI)
```

### 2. Prepare Data

**Option A: Download SEC 10-K Filings (Recommended)**
```bash
# Download ~100 SEC 10-K filings from major companies
python scripts/download_sec_filings.py
```

**Option B: Use Your Own Documents**
```bash
# Create directory and add your documents
mkdir -p data/documents
# Place your PDFs or text files here
```

### 3. Run the Application

```bash
# Start the chat interface
streamlit run apps/insurance_chat/app.py

# Or start the evaluation dashboard
streamlit run apps/insurance_chat/pages/eval_dashboard.py
```

### 4. Configure LLM Provider

**Option A: Ollama (Local, Free)**
```bash
# Install Ollama from https://ollama.ai
ollama pull llama3.2
ollama serve
```

**Option B: OpenAI (Cloud, Paid)**
```bash
# Set in .env
OPENAI_API_KEY=your_key_here
```

Then edit `configs/rag_config.yaml`:
```yaml
llm:
  provider: "openai"  # or "ollama"
  model: "gpt-4"      # or "llama3.2"
```

## ğŸ“Š Running Evaluations

### Golden Dataset Evaluation

```python
from src.models.rag_model import create_rag_model
from src.evals.rag_metrics import RAGEvaluator
from pathlib import Path

# Load model
model = create_rag_model(Path("data/documents"))

# Load evaluator
evaluator = RAGEvaluator()

# Load golden dataset
import pandas as pd
df = pd.read_csv("data/eval_datasets/golden_qa.csv")

# Run evaluation
test_cases = []
for _, row in df.iterrows():
    result = model.query(row["question"])
    test_cases.append({
        "query": row["question"],
        "response": result["answer"],
        "contexts": [node["text"] for node in result["source_nodes"]],
        "ground_truth": row["expected_answer"],
    })

metrics = evaluator.evaluate_batch(test_cases)
print(f"Overall Score: {metrics['overall_score']:.3f}")
```

### Adversarial Testing

```python
from src.evals.adversarial_tests import AdversarialTestSuite
from src.safety.input_validator import InputValidator
from src.safety.output_filter import OutputFilter

# Load safety components
input_validator = InputValidator()
output_filter = OutputFilter()

# Load test suite
suite = AdversarialTestSuite()

# Run all tests
results = []
for test_case in suite.get_all_tests():
    result = suite.run_test(test_case, model, input_validator, output_filter)
    results.append(result)

# Calculate safety metrics
from src.evals.safety_metrics import SafetyMetrics
safety_metrics = SafetyMetrics.from_test_results(results)
print(safety_metrics)
```

### Continuous Evaluation

```python
from src.evals.continuous_eval import create_continuous_evaluator

# Create evaluator
evaluator = create_continuous_evaluator(model)

# Setup schedule (weekly golden, nightly adversarial)
evaluator.setup_schedule()

# Run scheduler (blocking)
evaluator.run_scheduler()
```

## ğŸ›¡ï¸ Safety Features

### Input Validation
- Jailbreak attempt detection (16+ patterns)
- Medical diagnosis request blocking
- PII extraction attempt detection
- SQL injection & XSS prevention
- Length limits & sanitization

### Output Filtering
- Hallucination detection (LLM-as-judge)
- PII detection & redaction (Presidio)
- Medical advice detection
- Harmful content filtering

### Adversarial Test Suite
50+ test cases across 6 categories:
- **Jailbreak** (5 tests): Instruction override attempts
- **Hallucination** (4 tests): Non-existent policy queries
- **Ambiguous** (4 tests): Unclear questions
- **PII Extraction** (4 tests): Data exfiltration attempts
- **Medical Advice** (4 tests): Diagnosis/treatment requests
- **Edge Cases** (4 tests): Malformed/malicious input

## ğŸ“ˆ Metrics & Monitoring

### RAG Quality Metrics
- **Faithfulness**: Answer grounded in retrieved context
- **Answer Relevance**: Response addresses the question
- **Context Precision**: Relevant chunks retrieved
- **Context Recall**: All needed information retrieved
- **Latency**: Response time (target < 3s)

### Safety Metrics
- **Attack Success Rate (ASR)**: % of adversarial queries that succeed (target < 5%)
- **False Positive Rate (FPR)**: % of legitimate queries blocked (target < 10%)
- **Hallucination Rate**: % of responses with made-up facts (target < 10%)
- **PII Leakage Rate**: % of responses with PII (target = 0%)

### Success Targets
- âœ… Query latency < 3 seconds (p95)
- âœ… Answer relevance > 90%
- âœ… Jailbreak success rate < 5%
- âœ… Zero PII leakage

## ğŸ”§ Configuration

All configuration is in YAML files under `configs/`:

**`rag_config.yaml`** - RAG pipeline settings
```yaml
embeddings:
  model_name: "BAAI/bge-large-en-v1.5"
  device: "cpu"  # or "cuda"

chunking:
  chunk_size: 1024
  chunk_overlap: 200

retrieval:
  top_k: 10
  similarity_threshold: 0.7
  use_hybrid_search: true  # Semantic + BM25

llm:
  provider: "ollama"  # or "openai"
  model: "llama3.2"
  temperature: 0.1
```

**`safety_config.yaml`** - Safety layer settings
```yaml
enable_input_validation: true
enable_output_filtering: true
enable_pii_detection: true

jailbreak_threshold: 0.7
hallucination_threshold: 0.8
max_query_length: 2000
```

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test file
pytest tests/test_safety.py
```

## ğŸ“ Golden Dataset

The project includes a curated dataset of 50 insurance Q&A pairs:
- **Easy** (15): Basic terminology and coverage
- **Medium** (20): Multi-step reasoning, comparisons
- **Hard** (15): Complex scenarios, edge cases

Categories:
- Basic coverage & terminology
- Pharmacy & medications
- Network coverage & providers
- Cost sharing (deductibles, copays)
- Legal requirements & protections
- Special services (telehealth, mental health)

## ğŸ”„ RAG vs Fine-tuning Comparison

Framework supports comparing RAG against fine-tuned models across:

| Dimension | RAG Advantage | Fine-tuned Advantage |
|-----------|---------------|----------------------|
| **Accuracy** | Varies | Often higher |
| **Latency** | 2-3s | < 1s |
| **Cost** | $2/1K queries | $15/1K queries |
| **Freshness** | âœ… Instant updates | âŒ Requires retraining |
| **Safety** | âœ… Better control | âŒ Harder to control |

## ğŸš§ Extending the System

### Adding New Document Types

```python
# Extend InsuranceDocumentLoader in src/rag/document_loader.py
# Add custom metadata extraction logic
```

### Custom Evaluation Metrics

```python
# Create new evaluator in src/evals/
class CustomEvaluator:
    def evaluate(self, query, response, contexts):
        # Your custom logic
        return score
```

### New Adversarial Tests

```python
# Add to ADVERSARIAL_TEST_SUITE in src/evals/adversarial_tests.py
AdversarialTestCase(
    id="custom_01",
    category="custom",
    query="Your test query",
    expected_behavior=ExpectedBehavior.REJECT,
    severity=TestSeverity.HIGH,
)
```

## ğŸ“š Key Dependencies

- **LlamaIndex**: RAG framework
- **ChromaDB**: Vector store
- **Sentence Transformers**: BGE-Large embeddings
- **Presidio**: PII detection
- **Streamlit**: UI framework
- **Pydantic**: Configuration validation
- **Plotly**: Metrics visualization

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

## ğŸ™ Acknowledgments

- Built on experiments from `notebooks/` (preserved for reference)
- Inspired by RAGAS evaluation framework
- Safety patterns from Anthropic, OpenAI safety research
- Insurance domain knowledge from public policy documents

## ğŸ“ Contact

- **LinkedIn**: [linkedin.com/in/gowsiyashek](http://www.linkedin.com/in/gowsiyashek)
- **Project Issues**: [GitHub Issues](https://github.com/yourusername/rag-eval-lab/issues)

---

**Note**: This system provides general insurance policy information, not personalized advice. Always consult with licensed insurance professionals for specific coverage questions.
